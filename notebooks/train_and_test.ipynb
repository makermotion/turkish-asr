{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/scutum/envs/data-science/lib/python3.8/site-packages/torchaudio/backend/utils.py:53: UserWarning: \"sox\" backend is being deprecated. The default backend will be changed to \"sox_io\" backend in 0.8.0 and \"sox\" backend will be removed in 0.9.0. Please migrate to \"sox_io\" backend. Please refer to https://github.com/pytorch/audio/issues/903 for the detail.\n  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import torch\n",
    "import torchaudio\n",
    "import librosa\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "from datasets import ClassLabel\n",
    "from datasets import load_dataset, load_metric\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "from transformers import Wav2Vec2CTCTokenizer, Wav2Vec2Processor, Wav2Vec2FeatureExtractor, Wav2Vec2ForCTC, Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'load_dataset' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-790a48c413cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcommon_voice_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"common_voice\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"tr\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"train+validation\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcommon_voice_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"common_voice\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"tr\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"test\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "common_voice_train = load_dataset(\"common_voice\", \"tr\", split=\"train+validation\")\n",
    "common_voice_test = load_dataset(\"common_voice\", \"tr\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_random_elements(dataset, num_examples=10):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'common_voice_train' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-f5073b67fad6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcommon_voice_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'common_voice_train' is not defined"
     ]
    }
   ],
   "source": [
    "common_voice_train.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_voice_train = common_voice_train.remove_columns(column_names=['accent', 'age', 'client_id', 'down_votes', 'gender', 'locale', 'segment', 'up_votes'])\n",
    "common_voice_test = common_voice_test.remove_columns(column_names=['accent', 'age', 'client_id', 'down_votes', 'gender', 'locale', 'segment', 'up_votes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/scutum/.cache/huggingface/datasets/downloads/extracted/56b9d11bf987fa5ac213dcaa581cb6dfe16abd5b7094815f0a07ae261121f12c/cv-corpus-6.1-2020-12-11/tr/clips/common_voice_tr_18731995.mp3</td>\n",
       "      <td>Ya tahmin hatalı çıkarsa?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/scutum/.cache/huggingface/datasets/downloads/extracted/56b9d11bf987fa5ac213dcaa581cb6dfe16abd5b7094815f0a07ae261121f12c/cv-corpus-6.1-2020-12-11/tr/clips/common_voice_tr_19170901.mp3</td>\n",
       "      <td>Komisyon çifte vatandaşlığı da tartışıyor.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/scutum/.cache/huggingface/datasets/downloads/extracted/56b9d11bf987fa5ac213dcaa581cb6dfe16abd5b7094815f0a07ae261121f12c/cv-corpus-6.1-2020-12-11/tr/clips/common_voice_tr_17427247.mp3</td>\n",
       "      <td>Ardından kilisede evlenme töreni yapılıyor.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/home/scutum/.cache/huggingface/datasets/downloads/extracted/56b9d11bf987fa5ac213dcaa581cb6dfe16abd5b7094815f0a07ae261121f12c/cv-corpus-6.1-2020-12-11/tr/clips/common_voice_tr_18756243.mp3</td>\n",
       "      <td>Türkiye tutuklamaları memnuniyetle karşıladı.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/home/scutum/.cache/huggingface/datasets/downloads/extracted/56b9d11bf987fa5ac213dcaa581cb6dfe16abd5b7094815f0a07ae261121f12c/cv-corpus-6.1-2020-12-11/tr/clips/common_voice_tr_18169121.mp3</td>\n",
       "      <td>Batı Balkanlar nereye doğru gidiyor?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>/home/scutum/.cache/huggingface/datasets/downloads/extracted/56b9d11bf987fa5ac213dcaa581cb6dfe16abd5b7094815f0a07ae261121f12c/cv-corpus-6.1-2020-12-11/tr/clips/common_voice_tr_17348439.mp3</td>\n",
       "      <td>Şirket bitmiş ürünlerini satmakta başarılı.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>/home/scutum/.cache/huggingface/datasets/downloads/extracted/56b9d11bf987fa5ac213dcaa581cb6dfe16abd5b7094815f0a07ae261121f12c/cv-corpus-6.1-2020-12-11/tr/clips/common_voice_tr_17953928.mp3</td>\n",
       "      <td>Ancak Nikoliç devam etmekte kararlı.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>/home/scutum/.cache/huggingface/datasets/downloads/extracted/56b9d11bf987fa5ac213dcaa581cb6dfe16abd5b7094815f0a07ae261121f12c/cv-corpus-6.1-2020-12-11/tr/clips/common_voice_tr_17559437.mp3</td>\n",
       "      <td>\"Makedonya üyelik başvurusunu yirmi iki Mart'ta sundu.\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>/home/scutum/.cache/huggingface/datasets/downloads/extracted/56b9d11bf987fa5ac213dcaa581cb6dfe16abd5b7094815f0a07ae261121f12c/cv-corpus-6.1-2020-12-11/tr/clips/common_voice_tr_21317254.mp3</td>\n",
       "      <td>Sergide yüz elli taslak ve kolaj çalışması yer alıyor.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>/home/scutum/.cache/huggingface/datasets/downloads/extracted/56b9d11bf987fa5ac213dcaa581cb6dfe16abd5b7094815f0a07ae261121f12c/cv-corpus-6.1-2020-12-11/tr/clips/common_voice_tr_17341280.mp3</td>\n",
       "      <td>Bunu kanıtlayacak pek fazla delil yok.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(common_voice_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "chars_to_ignore_regex = '[\\,\\?\\.\\!\\-\\;\\:\\\"\\“\\%\\‘\\”\\�]'\n",
    "\n",
    "def remove_special_characters(batch):\n",
    "    batch[\"sentence\"] = re.sub(chars_to_ignore_regex, '', batch[\"sentence\"]).lower() + \" \"\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/scutum/.cache/huggingface/datasets/common_voice/tr/6.1.0/0041e06ab061b91d0a23234a2221e87970a19cf3a81b20901474cffffeb7869f/cache-f62dcd4947ab6451.arrow\n",
      "Loading cached processed dataset at /home/scutum/.cache/huggingface/datasets/common_voice/tr/6.1.0/0041e06ab061b91d0a23234a2221e87970a19cf3a81b20901474cffffeb7869f/cache-92d876638c917d70.arrow\n"
     ]
    }
   ],
   "source": [
    "common_voice_train = common_voice_train.map(remove_special_characters)\n",
    "common_voice_test = common_voice_test.map(remove_special_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all_chars(batch):\n",
    "    all_text = \" \".join(batch[\"sentence\"])\n",
    "    vocab = list(set(all_text))\n",
    "    return {\"vocab\": [vocab], \"all_text\": [all_text]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df14d8d2b3b343d9b7a3ee55ad886044",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ca0cca2172c460bb3c51c24c9d914da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "vocab_train = common_voice_train.map(extract_all_chars, batched=True, batch_size=-1, keep_in_memory=True, remove_columns=common_voice_train.column_names)\n",
    "vocab_test = common_voice_test.map(extract_all_chars, batched=True, batch_size=-1, keep_in_memory=True, remove_columns=common_voice_test.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_list = list(set(vocab_train[\"vocab\"][0]) | set(vocab_test['vocab'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dict = {v: k for k, v in enumerate(vocab_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dict['|'] = vocab_dict[' ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "del vocab_dict[' ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_dict[\"[UNK]\"] = len(vocab_dict)\n",
    "vocab_dict[\"[PAD]\"] = len(vocab_dict)\n",
    "len(vocab_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vocab.json', 'w') as vocab_file:\n",
    "    json.dump(vocab_dict, vocab_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Wav2Vec2CTCTokenizer(\"./vocab.json\", unk_token=\"[UNK]\", pad_token=\"[PAD]\", word_delimiter_token=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = Wav2Vec2FeatureExtractor(feature_size=1, sampling_rate=16000, padding_value=0.0, do_normalize=True, return_attention_mask=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.save_pretrained(\"path/to/folder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***"
   ]
  },
  {
   "source": [
    "### Load a custom audio file. // Trial"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sa, sr = torchaudio.load('/home/scutum/covost_tr/tr/raw/clips/common_voice_tr_20210689.mp3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(torch.Size([1, 161280]), 48000)"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "sa.shape, sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -1.8120e-05,\n",
       "         -3.4273e-07,  2.7820e-05]])"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "sa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "type(sa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(161280,)"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "np.asarray(sa.reshape(-1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sa = librosa.resample(np.asarray(sa.reshape(-1)), 44_100, 16_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(58515,)"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "sa.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def speech_file_to_array_fn(batch):\n",
    "    speech_array, sampling_rate = torchaudio.load(batch[\"path\"])\n",
    "    batch[\"speech\"] = speech_array[0].numpy()\n",
    "    batch[\"sampling_rate\"] = sampling_rate\n",
    "    batch[\"target_text\"] = batch[\"sentence\"]\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/scutum/.cache/huggingface/datasets/common_voice/tr/6.1.0/0041e06ab061b91d0a23234a2221e87970a19cf3a81b20901474cffffeb7869f/cache-c3e35f3c1c73eac0.arrow\n"
     ]
    }
   ],
   "source": [
    "common_voice_train = common_voice_train.map(speech_file_to_array_fn, remove_columns=common_voice_train.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/scutum/.cache/huggingface/datasets/common_voice/tr/6.1.0/0041e06ab061b91d0a23234a2221e87970a19cf3a81b20901474cffffeb7869f/cache-bdb2cb17a36f2336.arrow\n"
     ]
    }
   ],
   "source": [
    "common_voice_test = common_voice_test.map(speech_file_to_array_fn, remove_columns=common_voice_test.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample(batch):\n",
    "    batch[\"speech\"] = librosa.resample(np.asarray(batch[\"speech\"]), 48_000, 16_000)\n",
    "    batch[\"sampling_rate\"] = 16_000\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/scutum/.cache/huggingface/datasets/common_voice/tr/6.1.0/0041e06ab061b91d0a23234a2221e87970a19cf3a81b20901474cffffeb7869f/cache-5d2f8e9dc351e062.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/scutum/.cache/huggingface/datasets/common_voice/tr/6.1.0/0041e06ab061b91d0a23234a2221e87970a19cf3a81b20901474cffffeb7869f/cache-affae8bd42bd86b6.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/scutum/.cache/huggingface/datasets/common_voice/tr/6.1.0/0041e06ab061b91d0a23234a2221e87970a19cf3a81b20901474cffffeb7869f/cache-ad01a5a6695b7b76.arrow\n",
      "Loading cached processed dataset at /home/scutum/.cache/huggingface/datasets/common_voice/tr/6.1.0/0041e06ab061b91d0a23234a2221e87970a19cf3a81b20901474cffffeb7869f/cache-f107ca79b733d04a.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/scutum/.cache/huggingface/datasets/common_voice/tr/6.1.0/0041e06ab061b91d0a23234a2221e87970a19cf3a81b20901474cffffeb7869f/cache-bdf52846ed042bea.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/scutum/.cache/huggingface/datasets/common_voice/tr/6.1.0/0041e06ab061b91d0a23234a2221e87970a19cf3a81b20901474cffffeb7869f/cache-e8e222cad49b1b63.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/scutum/.cache/huggingface/datasets/common_voice/tr/6.1.0/0041e06ab061b91d0a23234a2221e87970a19cf3a81b20901474cffffeb7869f/cache-02f552ad1c408a56.arrow\n",
      "Loading cached processed dataset at /home/scutum/.cache/huggingface/datasets/common_voice/tr/6.1.0/0041e06ab061b91d0a23234a2221e87970a19cf3a81b20901474cffffeb7869f/cache-47aebb1244894886.arrow\n"
     ]
    }
   ],
   "source": [
    "common_voice_train = common_voice_train.map(resample, num_proc=4)\n",
    "common_voice_test = common_voice_test.map(resample, num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sampling_rate', 'speech', 'target_text']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_voice_train.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target text: oysa bu on yıl sonra gerçekleşti \n",
      "Input array shape: (54144,)\n",
      "Sampling rate: 16000\n"
     ]
    }
   ],
   "source": [
    "rand_int = random.randint(0, len(common_voice_train)-1)\n",
    "\n",
    "print(\"Target text:\", common_voice_train[rand_int][\"target_text\"])\n",
    "print(\"Input array shape:\", np.asarray(common_voice_train[rand_int][\"speech\"]).shape)\n",
    "print(\"Sampling rate:\", common_voice_train[rand_int][\"sampling_rate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(batch):\n",
    "    # check that all files have the correct sampling rate\n",
    "    assert (\n",
    "        len(set(batch[\"sampling_rate\"])) == 1\n",
    "    ), f\"Make sure all inputs have the same sampling rate of {processor.feature_extractor.sampling_rate}.\"\n",
    "\n",
    "    batch[\"input_values\"] = processor(batch[\"speech\"], sampling_rate=batch[\"sampling_rate\"][0]).input_values\n",
    "    \n",
    "    with processor.as_target_processor():\n",
    "        batch[\"labels\"] = processor(batch[\"target_text\"]).input_ids\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/scutum/.cache/huggingface/datasets/common_voice/tr/6.1.0/0041e06ab061b91d0a23234a2221e87970a19cf3a81b20901474cffffeb7869f/cache-1a19bf256dd9b52d.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/scutum/.cache/huggingface/datasets/common_voice/tr/6.1.0/0041e06ab061b91d0a23234a2221e87970a19cf3a81b20901474cffffeb7869f/cache-dcfa5d153ece4398.arrow\n",
      "Loading cached processed dataset at /home/scutum/.cache/huggingface/datasets/common_voice/tr/6.1.0/0041e06ab061b91d0a23234a2221e87970a19cf3a81b20901474cffffeb7869f/cache-a8561f4f748db8c2.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/scutum/.cache/huggingface/datasets/common_voice/tr/6.1.0/0041e06ab061b91d0a23234a2221e87970a19cf3a81b20901474cffffeb7869f/cache-bd31e74c1d66747b.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/scutum/.cache/huggingface/datasets/common_voice/tr/6.1.0/0041e06ab061b91d0a23234a2221e87970a19cf3a81b20901474cffffeb7869f/cache-d8f22c64a92e0180.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/scutum/.cache/huggingface/datasets/common_voice/tr/6.1.0/0041e06ab061b91d0a23234a2221e87970a19cf3a81b20901474cffffeb7869f/cache-ad8e5267b8c23cdd.arrow\n",
      "Loading cached processed dataset at /home/scutum/.cache/huggingface/datasets/common_voice/tr/6.1.0/0041e06ab061b91d0a23234a2221e87970a19cf3a81b20901474cffffeb7869f/cache-c96682cdd57de64e.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/scutum/.cache/huggingface/datasets/common_voice/tr/6.1.0/0041e06ab061b91d0a23234a2221e87970a19cf3a81b20901474cffffeb7869f/cache-195da893bc377574.arrow\n"
     ]
    }
   ],
   "source": [
    "common_voice_train = common_voice_train.map(prepare_dataset, remove_columns=common_voice_train.column_names, batch_size=8, num_proc=4, batched=True)\n",
    "common_voice_test = common_voice_test.map(prepare_dataset, remove_columns=common_voice_test.column_names, batch_size=8, num_proc=4, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs received.\n",
    "    Args:\n",
    "        processor (:class:`~transformers.Wav2Vec2Processor`)\n",
    "            The processor used for proccessing the data.\n",
    "        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n",
    "            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
    "            among:\n",
    "            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
    "              sequence if provided).\n",
    "            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
    "              maximum acceptable input length for the model if that argument is not provided.\n",
    "            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
    "              different lengths).\n",
    "        max_length (:obj:`int`, `optional`):\n",
    "            Maximum length of the ``input_values`` of the returned list and optionally padding length (see above).\n",
    "        max_length_labels (:obj:`int`, `optional`):\n",
    "            Maximum length of the ``labels`` returned list and optionally padding length (see above).\n",
    "        pad_to_multiple_of (:obj:`int`, `optional`):\n",
    "            If set will pad the sequence to a multiple of the provided value.\n",
    "            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n",
    "            7.5 (Volta).\n",
    "    \"\"\"\n",
    "\n",
    "    processor: Wav2Vec2Processor\n",
    "    padding: Union[bool, str] = True\n",
    "    max_length: Optional[int] = None\n",
    "    max_length_labels: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    pad_to_multiple_of_labels: Optional[int] = None\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lenghts and need\n",
    "        # different padding methods\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        with self.processor.as_target_processor():\n",
    "            labels_batch = self.processor.pad(\n",
    "                label_features,\n",
    "                padding=self.padding,\n",
    "                max_length=self.max_length_labels,\n",
    "                pad_to_multiple_of=self.pad_to_multiple_of_labels,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "wer_metric = load_metric(\"wer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    pred_logits = pred.predictions\n",
    "    pred_ids = np.argmax(pred_logits, axis=-1)\n",
    "\n",
    "    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    pred_str = processor.batch_decode(pred_ids)\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n",
    "\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Wav2Vec2ForCTC.from_pretrained(\n",
    "    \"facebook/wav2vec2-large-xlsr-53\", \n",
    "    attention_dropout=0.1,\n",
    "    hidden_dropout=0.1,\n",
    "    feat_proj_dropout=0.0,\n",
    "    mask_time_prob=0.05,\n",
    "    layerdrop=0.1,\n",
    "    gradient_checkpointing=True, \n",
    "    ctc_loss_reduction=\"mean\", \n",
    "    pad_token_id=processor.tokenizer.pad_token_id,\n",
    "    vocab_size=len(processor.tokenizer)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.freeze_feature_extractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "  output_dir=\"/home/scutum/wav2vec2-large-xlsr-turkish-demo\",\n",
    "  # output_dir=\"./wav2vec2-large-xlsr-turkish-demo\",\n",
    "  group_by_length=True,\n",
    "  per_device_train_batch_size=4,\n",
    "  gradient_accumulation_steps=2,\n",
    "  evaluation_strategy=\"steps\",\n",
    "  num_train_epochs=30,\n",
    "  fp16=True,\n",
    "  save_steps=100,\n",
    "  eval_steps=100,\n",
    "  logging_steps=100,\n",
    "  learning_rate=3e-4,\n",
    "  warmup_steps=500,\n",
    "  save_total_limit=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=common_voice_train,\n",
    "    eval_dataset=common_voice_test,\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Wav2Vec2ForCTC.from_pretrained(\"path/to/saved_model\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "processor = Wav2Vec2Processor.from_pretrained(\"path/to/saved_processor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It is strongly recommended to pass the ``sampling_rate`` argument to this function.Failing to do so can result in silent errors that might be hard to debug.\n"
     ]
    }
   ],
   "source": [
    "input_dict = processor(common_voice_test[\"input_values\"][2], return_tensors=\"pt\", padding=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "It is strongly recommended to pass the ``sampling_rate`` argument to this function.Failing to do so can result in silent errors that might be hard to debug.\n"
     ]
    }
   ],
   "source": [
    "sa = processor(sa, return_tensors='pt', padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'input_values': tensor([[ 0.0064,  0.0064,  0.0064,  ..., -0.0042,  0.0059,  0.0264]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]])}"
      ]
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "sa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = model(sa.input_values.to(\"cuda\")).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_ids = torch.argmax(logits, dim=-1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ışıkları açar mısın\n"
     ]
    }
   ],
   "source": [
    "print(processor.decode(pred_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = model(input_dict.input_values.to(\"cuda\")).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_ids = torch.argmax(logits, dim=-1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_voice_test_transcription = load_dataset(\"common_voice\", \"tr\", data_dir=\"./cv-corpus-6.1-2020-12-11\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Prediction:\")\n",
    "print(processor.decode(pred_ids))\n",
    "\n",
    "print(\"\\nReference:\")\n",
    "print(common_voice_test_transcription[2][\"sentence\"].lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('data-science')",
   "metadata": {
    "interpreter": {
     "hash": "351adceb4366b56d1ea9218106d4cf6e2e051aea3a6f3c7a47e972a2d8291b0b"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}